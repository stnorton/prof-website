{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Gensim for Topic Models in Social Science Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim is a fantastic Python module capable of handling large corpora of text data easier and faster than most the existing social science toolkit. In particular, Gensim is capable of parallelizing model fitting, while R packages cannot. This leads to R breaking when confronted with even trivially large amounts of text data. This is how I came across Gensim personally - a corpus with ~ 5 million documents was simply impossible to fit a topic model to using R's `topicmodel` package. Gensim is perfectly capable of handling corpora this large, and even doing so in constant memory!\n",
    "\n",
    "However, `gensim` seems to have been primarily written for enterprise use, and topic modeling is only one of an impressive array of different NLP models in the package. As such, I found it ocassionally difficult to extract the quantities of interest I needed for political science research. \n",
    "\n",
    "The purpose of this tutorial is to show other social scientists how to set-up gensim, run a parallelized LDA model, and extract some common quantities of interest from the models. I don't claim that the solutions I've found are necessarily the most optimal, and would love to hear from you if you improve on my code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up your environment\n",
    "\n",
    "Gensim runs best in a virtual environment. In particular, I ran into issues with parallelization when not isolated to a Conda environment; it seems to conflict with [something in scikit-learn](https://stackoverflow.com/questions/33929680/gensim-ldamulticore-not-multiprocessing). Regardless, I confine most my Python projects to Conda environments - the little bit of setup is worth not being caught in dependency hell. If you aren't already using Conda environments with Python, I *strongly* reccommend you start.\n",
    "\n",
    "I won't go into Conda environments in detail here. The Conda docs have a [fantastic tutorial](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html), and the `.yml` file for my environment is [available here](https://gist.github.com/stnorton/7a39cf0f54fbdb64d1c06967e74a1683). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "We're going to start with the imports necessary. There's nothing complicated her, but for more involved workflows you may need to import additional pre-processors from `nltk`, stemmers for different workflows, etc. We're going to use the spaCy model here because [it's awesome](https://spacy.io/), but since I mostly work with Russian data I never get a chance to use it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  #regular expression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint #pretty printing\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "#gensim imports\n",
    "import gensim #whole module\n",
    "import gensim.corpora as corpora #convenience rename\n",
    "from gensim.utils import simple_preprocess #import preprocessor\n",
    "from gensim.models import CoherenceModel #model for coherence\n",
    "\n",
    "#enable logging for gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "logging.root.level = logging.INFO  #ipython sometimes messes up the logging setup\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's download the toy dataset for this tutorial. Following [this tutorial](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/), which I learned the Gensim basics from, we're going to be using the 20-Newsgroup dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rec.autos' 'comp.sys.mac.hardware' 'comp.graphics' 'sci.space'\n",
      " 'talk.politics.guns' 'sci.med' 'comp.sys.ibm.pc.hardware'\n",
      " 'comp.os.ms-windows.misc' 'rec.motorcycles' 'talk.religion.misc'\n",
      " 'misc.forsale' 'alt.atheism' 'sci.electronics' 'comp.windows.x'\n",
      " 'rec.sport.hockey' 'rec.sport.baseball' 'soc.religion.christian'\n",
      " 'talk.politics.mideast' 'talk.politics.misc' 'sci.crypt']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  target  \\\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
       "\n",
       "            target_names  \n",
       "0              rec.autos  \n",
       "1  comp.sys.mac.hardware  \n",
       "2  comp.sys.mac.hardware  \n",
       "3          comp.graphics  \n",
       "4              sci.space  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read json file in\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "\n",
    "#print names of newsgroups\n",
    "print(df.target_names.unique())\n",
    "\n",
    "#inspect\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're at it, we need to download and load the basic English stop words from the `nltk` package. You'll also need to download the spaCy model if you don't already have it. Uncomment the code below and use it if you need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download nltk\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "#download spacy - run in terminal\n",
    "#python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now need to clean the data, using regular expressions. I borrow the code from the tutorial for this purpose. These regexs will remove the emails, the new line characters, and quote signs. We'll then use Gensim's `simple_preprocess()` function to get rid of punctuation and tokenize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: (wheres my thing) Subject: WHAT car is this!? Nntp-Posting-Host: '\n",
      " 'rac3.wam.umd.edu Organization: University of Maryland, College Park Lines: '\n",
      " '15 I was wondering if anyone out there could enlighten me on this car I saw '\n",
      " 'the other day. It was a 2-door sports car, looked to be from the late 60s/ '\n",
      " 'early 70s. It was called a Bricklin. The doors were really small. In '\n",
      " 'addition, the front bumper was separate from the rest of the body. This is '\n",
      " 'all I know. If anyone can tellme a model name, engine specs, years of '\n",
      " 'production, where this car is made, history, or whatever info you have on '\n",
      " 'this funky looking car, please e-mail. Thanks, - IL ---- brought to you by '\n",
      " 'your neighborhood Lerxst ---- ']\n",
      "[['from',\n",
      "  'wheres',\n",
      "  'my',\n",
      "  'thing',\n",
      "  'subject',\n",
      "  'what',\n",
      "  'car',\n",
      "  'is',\n",
      "  'this',\n",
      "  'nntp',\n",
      "  'posting',\n",
      "  'host',\n",
      "  'rac',\n",
      "  'wam',\n",
      "  'umd',\n",
      "  'edu',\n",
      "  'organization',\n",
      "  'university',\n",
      "  'of',\n",
      "  'maryland',\n",
      "  'college',\n",
      "  'park',\n",
      "  'lines',\n",
      "  'was',\n",
      "  'wondering',\n",
      "  'if',\n",
      "  'anyone',\n",
      "  'out',\n",
      "  'there',\n",
      "  'could',\n",
      "  'enlighten',\n",
      "  'me',\n",
      "  'on',\n",
      "  'this',\n",
      "  'car',\n",
      "  'saw',\n",
      "  'the',\n",
      "  'other',\n",
      "  'day',\n",
      "  'it',\n",
      "  'was',\n",
      "  'door',\n",
      "  'sports',\n",
      "  'car',\n",
      "  'looked',\n",
      "  'to',\n",
      "  'be',\n",
      "  'from',\n",
      "  'the',\n",
      "  'late',\n",
      "  'early',\n",
      "  'it',\n",
      "  'was',\n",
      "  'called',\n",
      "  'bricklin',\n",
      "  'the',\n",
      "  'doors',\n",
      "  'were',\n",
      "  'really',\n",
      "  'small',\n",
      "  'in',\n",
      "  'addition',\n",
      "  'the',\n",
      "  'front',\n",
      "  'bumper',\n",
      "  'was',\n",
      "  'separate',\n",
      "  'from',\n",
      "  'the',\n",
      "  'rest',\n",
      "  'of',\n",
      "  'the',\n",
      "  'body',\n",
      "  'this',\n",
      "  'is',\n",
      "  'all',\n",
      "  'know',\n",
      "  'if',\n",
      "  'anyone',\n",
      "  'can',\n",
      "  'tellme',\n",
      "  'model',\n",
      "  'name',\n",
      "  'engine',\n",
      "  'specs',\n",
      "  'years',\n",
      "  'of',\n",
      "  'production',\n",
      "  'where',\n",
      "  'this',\n",
      "  'car',\n",
      "  'is',\n",
      "  'made',\n",
      "  'history',\n",
      "  'or',\n",
      "  'whatever',\n",
      "  'info',\n",
      "  'you',\n",
      "  'have',\n",
      "  'on',\n",
      "  'this',\n",
      "  'funky',\n",
      "  'looking',\n",
      "  'car',\n",
      "  'please',\n",
      "  'mail',\n",
      "  'thanks',\n",
      "  'il',\n",
      "  'brought',\n",
      "  'to',\n",
      "  'you',\n",
      "  'by',\n",
      "  'your',\n",
      "  'neighborhood',\n",
      "  'lerxst']]\n"
     ]
    }
   ],
   "source": [
    "#convert data to list - required to run regexes on it using list comprehensions\n",
    "data = df.content.values.tolist()\n",
    "\n",
    "#remove emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "#remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "#get rid of single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "#simple preprocess\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_tok = list(sent_to_words(data))\n",
    "\n",
    "pprint(data_tok[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to remove stopwords, and then lemmatize/stem using spaCy and restrict the data to only nouns, adjectives, verbs, and adverbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['where',\n",
      "  's',\n",
      "  'thing',\n",
      "  'subject',\n",
      "  'car',\n",
      "  'nntp',\n",
      "  'post',\n",
      "  'host',\n",
      "  'rac',\n",
      "  'wam',\n",
      "  'umd',\n",
      "  'edu',\n",
      "  'organization',\n",
      "  'university',\n",
      "  'maryland',\n",
      "  'college',\n",
      "  'park',\n",
      "  'line',\n",
      "  'wonder',\n",
      "  'anyone',\n",
      "  'could',\n",
      "  'enlighten',\n",
      "  'car',\n",
      "  'see',\n",
      "  'day',\n",
      "  'door',\n",
      "  'sport',\n",
      "  'car',\n",
      "  'look',\n",
      "  'late',\n",
      "  'early',\n",
      "  'call',\n",
      "  'bricklin',\n",
      "  'door',\n",
      "  'really',\n",
      "  'small',\n",
      "  'addition',\n",
      "  'front',\n",
      "  'bumper',\n",
      "  'separate',\n",
      "  'rest',\n",
      "  'body',\n",
      "  'know',\n",
      "  'anyone',\n",
      "  'tellme',\n",
      "  'model',\n",
      "  'name',\n",
      "  'engine',\n",
      "  'spec',\n",
      "  'year',\n",
      "  'production',\n",
      "  'car',\n",
      "  'make',\n",
      "  'history',\n",
      "  'whatev',\n",
      "  'info',\n",
      "  'funky',\n",
      "  'look',\n",
      "  'car',\n",
      "  'mail',\n",
      "  'thank',\n",
      "  'bring',\n",
      "  'neighborhood',\n",
      "  'lerxst']]\n"
     ]
    }
   ],
   "source": [
    "#define functions\n",
    "def remove_stopwords(texts):\n",
    "    '''Loops over texts, preprocess, and then compares each word to stopwords using a list comprehension'''\n",
    "    for text in texts:\n",
    "        return[[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def stemmer(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    '''Uses spaCy EN model to stem words, remove certain parts of speech'''\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "#load in stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#instatiate spacy model, keeping only tagger\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "data_tok = remove_stopwords(data_tok)\n",
    "\n",
    "data_stem = stemmer(data_tok)\n",
    "\n",
    "pprint(data_stem[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LDA\n",
    "\n",
    "We're almost ready to run the model. At this point, we just need the dictionary and corpus, which will be fed to the LDA model. We use functions within Gensim to do this. `corpora.Dictionary` stores each unique word and assigns it a numeric ID. The `.doc2bow` method converts the corpus to its bag-of-words representation (also known as the term document frequency matrix, although Gensim doesn't store this as a matrix, but rather a list of tuples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary\n",
    "dictionary = corpora.Dictionary(data_stem)\n",
    "\n",
    "#full texts - for later reference\n",
    "texts = data_stem\n",
    "\n",
    "#bag of words\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move on to modeling. This corpus isn't actually big enough for parallelization to be worthwhile, but we'll do it for sake of demonstration.\n",
    "\n",
    "I won't get into too much detail on what the arguments to multicore mean - most of them are self-explanatory. Important for parallelization and working with big corpora, however, are `chunksize`, `workers`, and `passes`.\n",
    "\n",
    "* `chunksize`: Controls how large training \"chunks\" of the data are. Part of the reason Gensim works so well with large corpuses is that it uses online variational inference, allowing the model to run in constant memory. The larger the chunks, the quicker the model runs, but the more memory needed (in order to hold the chunk in memory). \n",
    "* `workers`: # of cores you have available to train the model. More cores, faster run.\n",
    "* `passes`: Since we're using variational inference, we need a stopping point for convergence. Gensim has a tolerance already set, but `passes` controls how many times the algorithm will attempt to reach convergence. If you're not reaching convergence on all/nearly all documents, you'll want to increase that number. See the gensim docs for more detail on this. Naturally, more passes increases the run time.\n",
    "\n",
    "We'll set up and run the model. If you want, you can also try the generic single core LDA model. It'll be faster in this case, since the overhead of setting up parallel processing isn't worth it on such a small corpus.\n",
    "\n",
    "**Make sure to set the random_state for replicability and set minimum_probability to 0 to prevent topics with low responbility from being filtered out of the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(corpus = corpus,\n",
    "                                      id2word = dictionary,\n",
    "                                      num_topics = 20, \n",
    "                                      random_state = 1017,\n",
    "                                      chunksize = 100,\n",
    "                                      passes = 10,\n",
    "                                      per_word_topics = True,\n",
    "                                      workers = 2, \n",
    "                                      minimum_probability = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the topics.The `print_topics()` method gives us the top tokens for each topic, plus its associated weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were tuning $k$ for a topic model, you'll need some measure to help you select the best $k$. Gensim has a built in [coherence model](https://radimrehurek.com/gensim/models/coherencemodel.html) for exactly that purpose, which allows you to choose between several different measures of coherence. This can also be parallelized using the `processes` argument, though I omit it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get c_v coherence score\n",
    "\n",
    "#instatiate model\n",
    "cm = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v') #processes = 2 would use 2 cpus\n",
    "\n",
    "#run model\n",
    "co = cm.get_coherence()\n",
    "\n",
    "print(co)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you have the basics of topic modeling in Gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harder Quantaties of Interest\n",
    "\n",
    "Everything up until this point has been remarkably easy. Now we're going to retrieve two quantaties of interest for which there is not a simple, pre-existing method: the most representative documents for each topic and topic assignments.\n",
    "\n",
    "Here, I'm defining the most representative documents as the $n$ documents with the highest cluster responsbility for a given topic (the cluster responsibilities are often known as Phi in the LDA literature). Topic assignments are a bit trickier to define, since LDA is a mixed-membership model - documents can belong to more than one topic. Nevertheless, it's often helpful to have clear-cut topic assignments in order to summarize your corpus or perform other secondary analyses. I assign topics here by simpling choosing the topic that best explains the document (highest responsibility), but there are other ways this could be done. Another method might be to set some baseline responsibility, say $1/k$, that the responsibility must be above in order to be assigned to a topic. This prevents assigning topics to documents the model was \"unsure\" about ($1/k$ being the probability of getting it right by random guessing). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Representative Documents\n",
    "\n",
    "We'll start with retrieving the 25 most representative documents for each topic. I'll show two methods here: one for the small model/if you have a lot of memory and another where you dump the responbilities to .csv to save memory. By dumping the responbilities to CSV, I was able to iterate over many models' outputs in a bash script, making it more memory efficient. Both methods work even if you forgot to specify the minimum probability argument (via the `fillna()` method).\n",
    "\n",
    "I do this using two functions. `lda_to_df` extracts cluster responsibilities to a pandas dataframe for ease of use - this probably isn't the most efficient way to do this, but it is simple. `get_best_docs` returns the $n$ most representative docs for each topic, given the output of `lda_to_df` as input. `lda_to_df` takes the corpus as input, whereas `get_best_docs` takes the full (unstemmed/uncleaned) texts as input. This could be easily combined into one function, but when working with large corpora or many models, it's convenient to save the intermediate output to file (as I do in the second example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no csv method\n",
    "\n",
    "#define functions\n",
    "def lda_to_df(model,corpus):\n",
    "    '''This function takes a gensim lda model as input, and outputs a df with topics probs by document'''\n",
    "    topic_probs = model.get_document_topics(corpus) #get the list of topic probabilities by doc\n",
    "    topic_dict = [dict(x) for x in topic_probs] #convert to dictionary to convert to data frame\n",
    "    df = pd.DataFrame(topic_dict).fillna(0) #convert to data frame\n",
    "    df['docs'] = df.index.values #create column with document indices (correspond to indices of dataframe)\n",
    "    df.columns = df.columns.astype(str) #convert to string to make indexing easier\n",
    "    return df\n",
    "\n",
    "def get_best_docs(df, n, k, texts):\n",
    "    '''Return the index of the n most representative documents from a list of topic responsibilities for each topic'''\n",
    "    '''n is the number of douments you want, k is the number of topics in the model, the texts are the FULL texts used to fit the model'''\n",
    "    #create column list to iterate over\n",
    "    k_cols = range(0, k)\n",
    "    \n",
    "    #intialize empty list to hold results\n",
    "    n_rep_docs = []\n",
    "    \n",
    "    #loop to extract documents for each topic\n",
    "    for i in k_cols:\n",
    "        inds = df.nlargest(n = n, columns = str(i))['docs'].astype(int).tolist()\n",
    "        #use list comprehension to extract documents\n",
    "        n_rep_docs.append([texts[ind] for ind in inds])\n",
    "    \n",
    "    return n_rep_docs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run functions\n",
    "resp = lda_to_df(lda_model, corpus)\n",
    "\n",
    "best_docs = get_best_docs(resp, 25, 20, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(best_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the function that outputs the responsibilities to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_to_csv(model, outfile, corpus):\n",
    "    '''This function takes a gensim lda model as input, and outputs a csv with topics probs by document'''\n",
    "    topic_probs = model.get_document_topics(corpus) #get the list of topic probabilities by doc\n",
    "    topic_dict = [dict(x) for x in topic_probs] #convert to dictionary to convert to data frame\n",
    "    df = pd.DataFrame(topic_dict).fillna(0) #convert to data frame, fill topics < 0.01 as 0\n",
    "    df.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Assignment\n",
    "\n",
    "This is a basic function that assigns topics based on the maximum cluster responsibility. Again, I'd like to emphasize that LDA models are mixed membership, and this isn't the only way to cut assignments of topics.\n",
    "\n",
    "This function operates diretly on the output of the `get_document_topics()` method, so it's more efficient than the function to retrieve the best documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_topic(model, corpus):\n",
    "    doc_topics = model.get_document_topics(corpus) #get by-topic probability for each document\n",
    "    topic_assignments = [] #initialize empty list for assignments\n",
    "    for i in range(len(doc_topics)): #loop over every document\n",
    "        doc = doc_topics[i] #extract relevant document\n",
    "        list_length = len(doc)\n",
    "        probs = []\n",
    "        for r in range(list_length):\n",
    "            probs.append(doc[r][1]) #get topic probs\n",
    "        max_val = max(probs) #get max value\n",
    "        max_ind = probs.index(max_val) #retrieve index\n",
    "        topic = doc[max_ind] #retrieve topic number\n",
    "        topic_assignments.append(topic[0]) #append only topic number (not also responsibility) to results\n",
    "    return topic_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments = assign_topic(lda_model, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 19, 11, 0, 0, 15, 0, 19, 0, 19]\n"
     ]
    }
   ],
   "source": [
    "pprint(assignments[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topic assignments are a simple list that you are free to use as you choose. If you also wanted to have the responsibilities, you could simply remove the slice from the `topic_assignments.append()` line in the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "There you have it - I hope that these functions are useful for you! Please get in touch if you have any questions or improve on my code!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gensimkern",
   "language": "python",
   "name": "gensimkern"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
